1. infra - terraform
2. unit test
3. pyspark or some compute?
4. centralized connection ? /( airflow connection)
5. parameterize test
6. val;idate                 [Extract]
                source="postgres"
                [Wait]

                [Load]
                destination="redshift"


7. eventually subdirectory should automatically be added to tag name
8. test needed to be added for airflow code genrated

9. one source, multi source, no source -> single destination
10. FOllow https://lakefs.io/blog/data-engineering-patterns-write-audit-publish/ WAP pattern


Design Guidelines
1. Data Factory to generate ETL pipeline
2. Simple to understand Metadata / TOML
3. WAP -> write audit publish pattern
4. Blue green deployment
5. unit, integrartion tests
6. Open Source , but Vendor Managed whereever possible
7. Dataset Driven scheduling or Time based